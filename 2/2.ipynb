{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<hr>\n",
    "<h1>INF-477. Redes Neuronales Artificiales.</h1>\n",
    "<h2>Tarea 2 - Autoencoders, RBMs y ConvNets</h2>\n",
    "<hr>\n",
    "</center>\n",
    "\n",
    "<div style=\"width:25%; display: inline-block\"></div>\n",
    "<div style=\"width:25%; display: inline-block\">\n",
    "    <b>Juan Carlos Garcés Bernt</b><br>\n",
    "    jcgarces@alumnos.inf.utfsm.cl\n",
    "</div>\n",
    "<div style=\"width:25%; display: inline-block;\">\n",
    "    <b>Natalia Gonzalez</b><br>\n",
    "    natalia.gonzalezg@usm.cl\n",
    "</div>\n",
    "<div style=\"width:25%; display: inline-block\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2 Aprendizaje Semi.Supervisado en NORB</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos discutido en clases, uno de los problemas más relevantes a la hora de aplicar técnicas de aprendizaje\n",
    "automático a problemas es reales es el requisito de disponer de un gran número de datos etiquetados,\n",
    "es decir, ejemplos para los que se conoce la respuesta deseada del sistema. Un problema de aprendizaje para\n",
    "el que existen pocos datos etiquetados y muchos datos no etiquetados se denomina *semi-supervisado*. En\n",
    "esta sección, utilizaremos la idea de pre-entrenar una red en modo no supervisado para atacar problemas\n",
    "de aprendizaje semi-supervisado. Con este objetivo en mente, trabajaremos con un dataset denominado\n",
    "$NORB$, introducido en [9] y utilizado en [10], que corresponde a imágenes estéreo de juguetes clasificados en\n",
    "$6$ categorías. Se tienen $291.600$ ejemplos de entrenamiento y $58.320$ ejemplos de pruebas.\n",
    "<img src=\"img/Fig2.png\">\n",
    "<center>Fig. 2: Dataset NORB.</center>\n",
    "Los datos asociados a esta actividad podrán ser obtenidos utilizando los siguientes comandos en la línea\n",
    "de comandos (sistemas UNIX)\n",
    "```\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_1\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_2\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_3\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_4\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_5\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_6\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_7\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_8\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_9\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_10\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_11\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_12\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_13\n",
    "```\n",
    "Los primeros $10$ batches corresponden a los datos de entrenamiento y los últimos $2$ a los datos de pruebas. Los\n",
    "archivos corresponden a diccionarios serializados de python y pueden ser \"extraídos\" utilizando la siguiente\n",
    "función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils \n",
    "from numpy.random import binomial\n",
    "\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, 'rb')\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict\n",
    "\n",
    "def codifica(Y):\n",
    "    for i in range(0,len(Y)):\n",
    "        Y[i] = np.array([Y[i]])\n",
    "    return np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez extraído, cada diccionario contendrá $2$ elementos importantes: *data* y *labels*. El primer elemento\n",
    "(*data*) es un matriz de $2048 \\times n$ (numpy array). Cada columna de esa matriz corresponde a una imagen\n",
    "estéreo de un juguete: los primeros $1024$ valores vienen de una de las cámaras/vistas y los siguientes $1024$ de\n",
    "la otra. Por otro lado, el elemento (*labels*) del diccionario contiene una lista de $n$ valores enteros entre $0$ y $5$ que identifican las clases antes a las que pertenecen los juguetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Construya una función que cargue todos los bloques de entrenamiento y pruebas del problema NORB\n",
    "generando como salida: <br>\n",
    "(i) dos matrices $X_{tr}, Y_{tr}$, correspondientes a las imágenes y etiquetas de entrenamiento, <br>\n",
    "(ii) dos matrices $X_t, Y_t$, correspondientes a las imágenes y etiquetas de pruebas, y finalmente<br>\n",
    "(iii) dos matrices $X_v, Y_v$, correspondientes a imágenes y etiquetas que se usarán como conjunto de\n",
    "validación, es decir para tomar decisiones de diseño acerca del modelo. Este último conjunto debe ser\n",
    "extraído desde el conjunto de entrenamiento seleccionando $5.832$ casos de cada batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_NORB_train(PATH):\n",
    "    xtr = []\n",
    "    ytr = []\n",
    "    \n",
    "    for b in range(1,11):\n",
    "        \n",
    "        f = os.path.join(PATH,'data_batch_%d' % (b, ))\n",
    "        datadict = unpickle(f)\n",
    "        X = datadict['data'].T\n",
    "        Y = datadict['labels']\n",
    "        Y = codifica(Y)\n",
    "        Z = np.concatenate((X,Y),axis=1)\n",
    "        del X,Y\n",
    "        np.random.shuffle(Z)\n",
    "        xtr.append(Z[5832:,0:-1])\n",
    "        ytr.append(Z[5832:,-1])\n",
    "\n",
    "        del Z\n",
    "      \n",
    "    Xtr = np.concatenate(xtr)\n",
    "    del xtr\n",
    "    Ytr = np.concatenate(ytr)\n",
    "    Y_tr = np_utils.to_categorical(Ytr, 6)\n",
    "    del ytr,Ytr\n",
    "    return Xtr, Y_tr\n",
    "    \n",
    "def load_NORB_val(PATH):\n",
    "    xval = []\n",
    "    yval = []\n",
    "    \n",
    "    for b in range(1,11):\n",
    "        \n",
    "        f = os.path.join(PATH,'data_batch_%d' % (b, ))\n",
    "        datadict = unpickle(f)\n",
    "        X = datadict['data'].T\n",
    "        Y = datadict['labels']\n",
    "        Y = codifica(Y)\n",
    "        Z = np.concatenate((X,Y),axis=1)\n",
    "        del X,Y\n",
    "        np.random.shuffle(Z)\n",
    "        xval.append(Z[:5832,0:-1])\n",
    "        yval.append(Z[:5832,-1])\n",
    "        del Z\n",
    "      \n",
    "    Xval = np.concatenate(xval)\n",
    "    del xval\n",
    "    Yval = np.concatenate(yval)\n",
    "    Y_val = np_utils.to_categorical(Yval, 6)\n",
    "    del yval,Yval\n",
    "\n",
    "    return Xval, Y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_NORB_test(PATH):\n",
    "    xts = []\n",
    "    yts = []\n",
    "    \n",
    "    for b in range(11,13):\n",
    "        f = os.path.join(PATH,'data_batch_%d' % (b, ))\n",
    "        datadict = unpickle(f)\n",
    "        X = datadict['data'].T\n",
    "        Y = datadict['labels']\n",
    "        Y = codifica(Y)\n",
    "        Z = np.concatenate((X,Y),axis=1)\n",
    "        del X,Y\n",
    "        np.random.shuffle(Z)\n",
    "        xts.append(Z[:,0:-1])\n",
    "        yts.append(Z[:,-1])\n",
    "        del Z\n",
    "    \n",
    "    Xts = np.concatenate(xts)\n",
    "    del xts\n",
    "    Yts = np.concatenate(yts)\n",
    "    Y_ts = np_utils.to_categorical(Yts, 6)\n",
    "    del yts,Yts\n",
    "    return Xts, Y_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Construya una función que escale apropiadamente las imágenes antes de trabajar. Experimente escalando\n",
    "linealmente los datos de tal forma que cada pixel quede en el intervalo $[-1,1]$ con el máximo y\n",
    "mínimo valor observado en los extremos del intervalo. Evalúe más tarde la ventaja de centrar y escalar\n",
    "los datos para que cada atributo (pixel) tenga desviación estándar $1$ y media nula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Escalamiento lineal con valores en [-1,1]\n",
    "def rescale_img(X):\n",
    "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    return X_scaled\n",
    "\n",
    "# Centrado y escalado con desviacion estandar 1 y media 0\n",
    "def stand_img(X):\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    X_scaled = scaler.transform(X)\n",
    "    return X_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Su objetivo será ahora evaluar el desempeño de una red FF en un escenario semi-supervisado. Para\n",
    "ello simulará un situación en la que se tienen $n_s$ ejemplos de entrenamiento para los cuales se conoce\n",
    "la etiqueta correcta y $n_{ns} = n_{tr} - n_s$ ejemplos para los cuales no se tiene esta información ($n_{tr}$ es el\n",
    "número total de ejemplos de entrenamiento). Para empezar, deberá entrenar una red FF con salida\n",
    "softmax para el problema NORB. Considere la inclusión de dos capas escondidas (de $4000$ y $2000$\n",
    "unidades) y funciones de activación $relu$. Como parámetros de referencia considere: BP con tasa de\n",
    "aprendizaje constante, función de pérdida *cross-entropy binaria*, y mini batches de tamaño $10$. Puede\n",
    "utilizar el conjunto de validación para mejorar el entrenamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pre_train_AE(Xtr,Xval,n_hidden_layer1,n_hidden_layer2):\n",
    "    #AE1\n",
    "    input_img1 = Input(shape=(2048,))\n",
    "    encoded1 = Dense(n_hidden_layer1,activation='relu')(input_img1)\n",
    "    decoded1 = Dense(2048, activation='relu')(encoded1)\n",
    "    autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "    encoder1 = Model(input=input_img1, output=encoded1)\n",
    "    autoencoder1.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "    autoencoder1.fit(Xtr, Xtr, nb_epoch=50, batch_size=20,shuffle=True, validation_data=(Xval, Xval))\n",
    "    encoded_input1 = Input(shape=(n_hidden_layer1,))\n",
    "\n",
    "    #AE2\n",
    "    x_train_encoded1 = encoder1.predict(Xtr)\n",
    "    x_val_encoded1 = encoder1.predict(Xval)\n",
    "    input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "    encoded2 = Dense(n_hidden_layer2, activation='relu')(input_img2)\n",
    "    decoded2 = Dense(n_hidden_layer1, activation='relu')(encoded2)\n",
    "    autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "    encoder2 = Model(input=input_img2, output=encoded2)\n",
    "    autoencoder2.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "    autoencoder2.fit(x_train_encoded1,x_train_encoded1,nb_epoch=50,batch_size=20,shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "    encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "    \n",
    "    w = []\n",
    "    w.append(autoencoder1.layers[1].get_weights())\n",
    "    w.append(autoencoder2.layers[1].get_weights())\n",
    "    return w\n",
    "    \n",
    "\n",
    "\n",
    "def pre_train_dAE(x_train,x_val,n_hidden_layer1,n_hidden_layer2):\n",
    "    ### NOISE \n",
    "    noise_level = 0.1\n",
    "    noise_mask = binomial(n=1,p=noise_level,size=x_train.shape)\n",
    "    noisy_x_train = x_train*noise_mask\n",
    "    noise_mask = binomial(n=1,p=noise_level,size=x_val.shape)\n",
    "    noisy_x_val = x_val*noise_mask\n",
    "\n",
    "    ###d_AUTOENCODER 1\n",
    "    input_img1 = Input(shape=(2048,))\n",
    "    encoded1 = Dense(n_hidden_layer1,activation='relu')(input_img1)\n",
    "    decoded1 = Dense(2048, activation='relu')(encoded1)\n",
    "    autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "    encoder1 = Model(input=input_img1, output=encoded1)\n",
    "    autoencoder1.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "    autoencoder1.fit(noisy_x_train, x_train, nb_epoch=50, batch_size=20,shuffle=True, validation_data=(noisy_x_val, x_val))\n",
    "    encoded_input1 = Input(shape=(n_hidden_layer1,))\n",
    "    \n",
    "    ###d_AUTOENCODER 2\n",
    "    x_train_encoded1 = encoder1.predict(noisy_x_train) \n",
    "    x_val_encoded1 = encoder1.predict(noisy_x_val)\n",
    "    input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "    encoded2 = Dense(n_hidden_layer2, activation='relu')(input_img2)\n",
    "    decoded2 = Dense(n_hidden_layer1, activation='relu')(encoded2)\n",
    "    autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "    encoder2 = Model(input=input_img2, output=encoded2)\n",
    "    autoencoder2.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "    autoencoder2.fit(x_train_encoded1,x_train_encoded1,nb_epoch=50,batch_size=20,shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "    encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "\n",
    "    w = []\n",
    "    w.append(autoencoder1.layers[1].get_weights())\n",
    "    w.append(autoencoder2.layers[1].get_weights())\n",
    "    return w\n",
    "    \n",
    "\n",
    "def pre_train_RBM(x_train,n_hidden_layer1,n_hidden_layer2):\n",
    "    \n",
    "    rbm1 = BernoulliRBM(n_components=n_hidden_layer1, batch_size=25,learning_rate=0.05,verbose=0, n_iter=50)\n",
    "    rbm1.fit(x_train)\n",
    "    encoded_train1 = rbm1.transform(x_train)\n",
    "    weights1 = []\n",
    "    weights1.append(rbm1.components_.T)\n",
    "    weights1.append(rbm1.intercept_hidden_.T)\n",
    "    \n",
    "    rbm2 = BernoulliRBM(n_components=n_hidden_layer2, batch_size=25,learning_rate=0.05,verbose=0, n_iter=50)\n",
    "    rbm2.fit(encoded_train1)\n",
    "    weights2 = []\n",
    "    weights2.append(rbm2.components_.T)\n",
    "    weights2.append(rbm1.intercept_hidden_.T)\n",
    "    \n",
    "    w = []\n",
    "    w.append(weights1)\n",
    "    w.append(weights2)\n",
    "    return w\n",
    "    \n",
    "    \n",
    "\n",
    "def ann_ff(Xtr,Ytr,Xval,Yval,Xts,Yts,scale=0,theta=1,pre_train='None',act_func='relu'):\n",
    "    n_hidden_layer1 = 4000\n",
    "    activation_layer1 = act_func\n",
    "    n_hidden_layer2 = 2000\n",
    "    activation_layer2 = act_func\n",
    "    optimizer_ = SGD(lr=1.0)\n",
    "    n_ns = int((1.0-theta)*len(Ytr))\n",
    "    \n",
    "    #########\n",
    "    # ESCALAMIENTO DE LAS IMAGENES\n",
    "    # scale = 0, no se escalan las imagenes\n",
    "    # scale = 1, se aplica escalamiento lineal\n",
    "    # scale = 2, se aplica estandarizacion de las imagenes (desv. estandar 1 y media nula)\n",
    "    escala = 'None'\n",
    "    if scale !=0:\n",
    "        if(scale == 1):\n",
    "            rescale_img(Xtr)\n",
    "            rescale_img(Xval)\n",
    "            rescale_img(Xts)\n",
    "            escala = 'Linear'\n",
    "        elif(scale == 2):\n",
    "            stand_img(Xtr)\n",
    "            stand_img(Xval)\n",
    "            stand_img(Xts)\n",
    "            escala = 'Standar Scale'\n",
    "        else:\n",
    "            return -1\n",
    "    ########\n",
    "    \n",
    "    ##########\n",
    "    ### PRE-ENTRENAMIENTO\n",
    "    if pre_train != 'None':\n",
    "        if pre_train == 'AE':\n",
    "            PreTrained = pre_train_AE(Xtr[:n_ns,:],Xval,n_hidden_layer1,n_hidden_layer2) \n",
    "        elif pre_train == 'dAE':\n",
    "            PreTrained = pre_train_dAE(Xtr[:n_ns,:],Xval,n_hidden_layer1,n_hidden_layer2)\n",
    "        elif pre_train == 'RBM':\n",
    "            PreTrained = pre_train_RBM(Xtr[:n_ns,:],n_hidden_layer1,n_hidden_layer2)\n",
    "        else:\n",
    "            return -1\n",
    "    ###########\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_hidden_layer1, activation=activation_layer1,input_shape=(2048,)))\n",
    "    if pre_train != 'None':\n",
    "        model.layers[-1].set_weights(PreTrained[0])\n",
    "    model.add(Dense(n_hidden_layer2, activation=activation_layer2))\n",
    "    if pre_train != 'None':\n",
    "        model.layers[-1].set_weights(PreTrained[1])\n",
    "\n",
    "    model.add(Dense(6, activation='softmax'))\n",
    "    model.summary()  \n",
    "    \n",
    "    model.compile(optimizer=optimizer_,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    \n",
    "    if pre_train != 'None':\n",
    "    model.fit(Xtr[n_ns:,:], Ytr[n_ns:], nb_epoch=0, batch_size=10,shuffle=True, validation_data=(Xval, Yval),verbose=1)\n",
    "    \n",
    "    else:\n",
    "        ################\n",
    "        #####Entrenamiento SemiSupervisado\n",
    "        n_ns = int((1.0-theta)*len(Ytr))\n",
    "        n_s = len(Ytr)- n_ns\n",
    "        N_NS = n_ns/32\n",
    "        N_S = n_s/32\n",
    "        print \"n_ns:\",n_ns,\" n_s:\",n_s,\" NS:\", N_S,\" NNS:\",N_NS\n",
    "        if n_ns == 0:\n",
    "            model.fit(Xtr, Ytr, nb_epoch=50, batch_size=20,shuffle=True, validation_data=(Xval, Yval),verbose=1)\n",
    "        else:\n",
    "            for i in range(0,32):\n",
    "                print \"fit: [\",N_NS*i+N_S*i,\":\",N_NS*(i)+N_S*(i+1),\"]\"\n",
    "                model.fit(Xtr[N_NS*i+N_S*i:N_NS*(i)+N_S*(i+1),:], Ytr[N_NS*i+N_S*i:N_NS*(i)+N_S*(i+1)], nb_epoch=15, batch_size=10,shuffle=True, validation_data=(Xval[:N_S,:], Yval[:N_S]),verbose=1)\n",
    "                Y_pred = np.array([])\n",
    "                print \"pred: [\",N_NS*(i)+N_S*(i+1),\":\",N_NS*(i+1)+N_S*(i+1),\"]\"\n",
    "                Y_pred = model.predict(Xtr[N_NS*(i)+N_S*(i+1):N_NS*(i+1)+N_S*(i+1),:])\n",
    "                model.fit(Xtr[N_NS*(i)+N_S*(i+1):N_NS*(i+1)+N_S*(i+1),:], Y_pred, nb_epoch=15, batch_size=10,shuffle=True, validation_data=(Xval[:N_S,:], Yval[:N_S]),verbose=1)\n",
    "        ################\n",
    "    \n",
    "    model.save(act_func+'Net-'+escala+'_scale-'+str(theta)+'_theta-'+pre_train+'_pretrain.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluador(X,Y,scale=0,theta=1,pre_train='None',act_func='relu'):\n",
    "    escala = 'None'\n",
    "    if(scale == 1): \n",
    "        escala = 'Linear'\n",
    "    elif(scale == 2): \n",
    "        escala = 'Standar Scale'\n",
    "    \n",
    "    ann = act_func+'Net-'+escala+'_scale-'+str(theta)+'_theta-'+pre_train+'_pretrain.h5'\n",
    "    model = load_model(ann)\n",
    "    score = model.evaluate(X, Y, verbose=0)\n",
    "    print \"Theta:\",theta,\" ---- Scale:\", escala\n",
    "    print \"Activation function:\", act_func,\" ---- Pre-Train:\", pre_train\n",
    "    print \"Test loss:\", score[0]\n",
    "    print \"Test accuracy:\", score[1]\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def graficador(A,B,titulo='_',y_label='_'):\n",
    "    plt.plot(A,B)\n",
    "    plt.title(titulo)\n",
    "    plt.xlabel('theta')\n",
    "    plt.ylabel(y_label)\n",
    "    plt.show()\n",
    "\n",
    "Xtr, Ytr = load_NORB_train(\".\")\n",
    "Xval, Yval = load_NORB_val(\".\")\n",
    "\n",
    "Xts, Yts = load_NORB_test(\".\")\n",
    "\n",
    "Theta = np.linspace(0,1,num=11)\n",
    "PT = ['None', 'AE', 'dAE', 'RBM'] \n",
    "AF = ['relu', 'tanh', 'sigmoid']\n",
    "\n",
    "#### Generando las ff ######\n",
    "for act_func_ in AF:\n",
    "    for pre_train_ in PT:\n",
    "        for scale_ in range(0,3):\n",
    "            for theta_ in Theta:\n",
    "                ann_ff(Xtr,Ytr,Xval,Yval,Xts,Yts,scale=scale_,theta=theta_,pre_train=pre_train_,act_func=act_func_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta: 1  ---- Scale: None\n",
      "Activation function: relu  ---- Pre-Train: None\n",
      "Test loss: 4.45284479596\n",
      "Test accuracy: 0.722222221797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.4528447959648725, 0.72222222179705875]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xts, Yts = load_NORB_test(\".\")\n",
    "evaluador(Xts,Yts,scale=0,theta=1,pre_train='None',act_func='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construya un gráfico que muestre\n",
    "cómo evoluciona el error de pruebas como función de $\\theta_s = n_s/n_{tr}$. Experimente con $\\theta_s = 0.1, 0.2,...1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for esc in range(0,3):\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='None',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Entrenamiento SemiSupervisado: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Entrenamiento SemiSupervisado: theta v/s accuracy',y_label='accuracy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Su objetivo será ahora construir un gráfico similar al anterior que muestre cómo evoluciona el error de pruebas como función de $\\theta_s = n_s / n_{tr}$ cuando la red se pre-entrena utilizando los datos no supervisados. ¿Mejora el resultado con respecto a la red entrenada utilizando sólo los casos para los que se conoce la etiqueta? Experimente pre-entrenando con distintas estrategias ( por ejemplo AE's versus dAe's ó AE's versus RBM's)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todos etiquetados y sin pre-entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for esc in range(0,3):\n",
    "    score = evaluador(Xts,Yts,scale=esc,theta=1,pre_train='None',act_func=funcion)\n",
    "    print \"loss=\",score[0]\n",
    "    print \"accuracy=\",score[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variando theta y con pre-entrenamiento AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for esc in range(0,3):\t\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='AE',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Pre-entrenamiento AE: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Pre-entrenamiento AE: theta v/s accuracy',y_label='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variando theta y con pre-entrenamiento dAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for esc in range(0,3):\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='dAE',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Pre-entrenamiento dAE: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Pre-entrenamiento dAE: theta v/s accuracy',y_label='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variando theta y con pre-entrenamiento RBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for esc in range(0,3):\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='RBM',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Pre-entrenamiento RBM: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Pre-entrenamiento RBM: theta v/s accuracy',y_label='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Repita el experimento anterior cambiando las funciones de activación a $sigmoidales$ y $tanh$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Activación Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "funcion = 'sigmoid'\n",
    "#c)\n",
    "for esc in range(0,3):\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='None',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Entrenamiento SemiSupervisado: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Entrenamiento SemiSupervisado: theta v/s accuracy',y_label='accuracy')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#d)\n",
    "# Todos etiquetados y sin pre-entrenamiento\n",
    "for esc in range(0,3):\n",
    "    score = evaluador(Xts,Yts,scale=esc,theta=1,pre_train='None',act_func=funcion)\n",
    "    print \"loss=\",score[0]\n",
    "    print \"accuracy=\",score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variando theta y con pre-entrenamiento AE\n",
    "for esc in range(0,3):\t\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='AE',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Pre-entrenamiento AE: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Pre-entrenamiento AE: theta v/s accuracy',y_label='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variando theta y con pre-entrenamiento dAE\n",
    "for esc in range(0,3):\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='dAE',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Pre-entrenamiento dAE: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Pre-entrenamiento dAE: theta v/s accuracy',y_label='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variando theta y con pre-entrenamiento RBM\n",
    "for esc in range(0,3):\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='RBM',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Pre-entrenamiento RBM: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Pre-entrenamiento RBM: theta v/s accuracy',y_label='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Activación Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "funcion = 'tanh'\n",
    "#c)\n",
    "for esc in range(0,3):\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='None',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Entrenamiento SemiSupervisado: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Entrenamiento SemiSupervisado: theta v/s accuracy',y_label='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#d)\n",
    "# Todos etiquetados y sin pre-entrenamiento\n",
    "for esc in range(0,3):\n",
    "    score = evaluador(Xts,Yts,scale=esc,theta=1,pre_train='None',act_func=funcion)\n",
    "    print \"loss=\",score[0]\n",
    "    print \"accuracy=\",score[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variando theta y con pre-entrenamiento AE\n",
    "for esc in range(0,3):\t\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='AE',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Pre-entrenamiento AE: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Pre-entrenamiento AE: theta v/s accuracy',y_label='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variando theta y con pre-entrenamiento dAE\n",
    "for esc in range(0,3):\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='dAE',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Pre-entrenamiento dAE: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Pre-entrenamiento dAE: theta v/s accuracy',y_label='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Variando theta y con pre-entrenamiento RBM\n",
    "for esc in range(0,3):\n",
    "    loss_ = []\n",
    "    accuracy_ = []\n",
    "    for i in range(1,11):\n",
    "        score = evaluador(Xts,Yts,scale=esc,theta=Theta[i],pre_train='RBM',act_func=funcion)\n",
    "        loss_.append(score[0])\n",
    "        accuracy_.append(score[1])\n",
    "\n",
    "    graficador(Theta,loss_,titulo='Pre-entrenamiento RBM: theta v/s loss',y_label='loss')\n",
    "    graficador(Theta,accuracy_,titulo='Pre-entrenamiento RBM: theta v/s accuracy',y_label='accuracy')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
