{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<hr>\n",
    "<h1>INF-477. Redes Neuronales Artificiales.</h1>\n",
    "<h2>Tarea 2 - Autoencoders, RBMs y ConvNets</h2>\n",
    "<hr>\n",
    "</center>\n",
    "\n",
    "<div style=\"width:25%; display: inline-block\"></div>\n",
    "<div style=\"width:25%; display: inline-block\">\n",
    "    <b>Juan Carlos Garcés Bernt</b><br>\n",
    "    jcgarces@alumnos.inf.utfsm.cl\n",
    "</div>\n",
    "<div style=\"width:25%; display: inline-block;\">\n",
    "    <b>Natalia Gonzales</b><br>\n",
    "    a@a.cl\n",
    "</div>\n",
    "<div style=\"width:25%; display: inline-block\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>1 Entrenamiento de Autoencoders (AEs) y RBMs en MNIST</h1>\n",
    "\n",
    "Como hemos discutido en clases, las RBM's y posteriormente los AE's, fueron un componente crucial en el\n",
    "desarrollo de los modelos que entre 2006 y 2010 vigorizaron el área de las redes neuronales artificiales con\n",
    "logros notables de desempeño en diferentes tareas de aprendizaje automático.<br>\n",
    "En esta sección aprenderemos a utilizar estos modelos en tres escenarios clásicos: reducción de dimensionalidad,\n",
    "denoising y pre-entrenamiento. Con este objetivo en mente, utilizaremos un dataset denominado\n",
    "*MNIST*, bastante conocido en el área e introducido por Yann LeCunn hacia 1998 en un trabajo que,\n",
    "junto al Neocognitron de Fukushima, se considera uno de los principales antecedentes de las redes convolucionales\n",
    "modernas. Se trata de una colección de $70.000$ imágenes de $28 \\times 28$ pixeles correspondientes a\n",
    "dígitos manuscritos (números entre $0$ y $9$). En su versión tradicional, la colección se encuentra separada en\n",
    "dos subconjuntos: uno de entrenamiento de $60.000$ imágenes y otro de test de $10.000$ imágenes. La tarea\n",
    "consiste en entrenar un programa para que aprenda a identificar correctamente el dígito representado en la\n",
    "imagen.\n",
    "<img src=\"img/Fig1.png\">\n",
    "Fig. 1: Dataset MNIST y visualización obtenida usando las primeras dos componentes principales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Escriba una función que cargue los datos desde el repositorio de keras, normalice las imágenes de\n",
    "modo que los pixeles queden en $[0, 1]$, transforme las imágenes en vectores $(\\in \\mathbb{R}^{784})$ y devuelva tres\n",
    "subconjuntos disjuntos: uno de entrenamiento, uno de validación y uno de pruebas. La normalización\n",
    "permite interpretar cada valor como una probabilidad de \"activación\" del un pixel. El conjunto de\n",
    "pruebas será aquel por defecto. Para la construcción del subconjunto de validación su función recibirá\n",
    "un parámetro **NVAL**, cuyo valor por defecto será $1000$. El conjunto de validación se construirá utilizando\n",
    "los últimos **NVAL** casos del conjunto del entrenamiento por defecto. El conjunto de entrenamiento\n",
    "consistirá en las primeras $60000$ - **NVAL** imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "x_val = x_train[-nval:]\n",
    "y_val = y_train[-nval:]\n",
    "x_train = x_train[:-nval]\n",
    "y_train = y_train[:-nval]\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_val = np_utils.to_categorical(y_val, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.1 Reducción de Dimensionalidad</h2>\n",
    "Construir una representación de menor dimensionalidad de un objeto en $\\mathbb{R}^d$, consiste en construir una función\u001e",
    " $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}^{d'},$ con $d' \\ll d$ \u001c",
    "que preserve lo mejor posible la \"información\" original. Obtener tal representación es útil desde un punto de vista computacional (compresión) y estadístico (permite construir modelos con un menor número de parámetros libres). Una técnica de reducción de dimensionalidad se denomina *no super-visada* cuando no hace uso de información acerca de las clases a las que pertenecen los datos de entrenamiento, marco de trabajo útil cuando dicha información no está disponible. Un AE y una RBM se pueden considerar métodos no-supervisados de reducción de dimensionalidad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Entrene un AR básico (1capa escondida) para generar una representación de MNIST en $d' = 2,8,32,64$ dimensiones. Determine el porcentaje de compresión obtenido y el error de recontrucción en cada caso.\n",
    "¿Mejora el resultado si elegimos una función de activación ReLU para el Encoder? ¿Podría utilizarse una ReLU en el decoder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(32, activation='sigmoid')(input_img)\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input=input_img, output=decoded)\n",
    "encoder = Model(input=input_img, output=encoded)\n",
    "encoded_input = Input(shape=(32,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(input=encoded_input, output=decoder_layer(encoded_input))\n",
    "autoencoder.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train,x_train,nb_epoch=50,batch_size=25,shuffle=True,validation_data=(x_val, x_val))\n",
    "autoencoder.save('basic_autoencoder_768x32.h5')\n",
    "#save other stuff ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Para verificar la calidad del modelo obtenido, compare visualmente la recontruscción que logra hacer el autoencoder desde la representación en $\\mathbb{R}^{d'}$ para algunas imágenes del conjunto de pruebas. Determine si la percepción visual se correpsonde con el error de recontrucción observado. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "autoencoder = load_model('basic_autoencoder_768x32.h5')\n",
    "#load other stuff ...\n",
    "encoded_test = encoder.predict(x_test)\n",
    "decoded_test = decoder.predict(encoded_test)\n",
    "import matplotlib\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Para verificar la calidad de la representación obtenida, implemente el siguiente clasificador, denominado *kNN* (k-nearest neighbor): dada una imagen $x$, el clasificador busca las $k=10$ imágenes de entrenamiento más similares $N_{x} = \\{ x^{(k_i)}\\}_{i=1}^{10}$ (de acuerdo a una distancia, e.g. euclidiana) y predice como clase, la etiqueta más popular entre las imágenes $N_x$. Mida el error de pruebas obtenido construyendo este clasificador sobre la data original y luego sobre la data reducida. Compare además los tiempos medios de predicción en ambos escenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_train = encoder.predict(x_train)\n",
    "encoded_test = encoder.predict(x_test)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(10)\n",
    "clf.fit(encoded_train, y_train)\n",
    "clf.fit(encoded_train, y_train)\n",
    "score = clf.score(encoded_test,y_test)\n",
    "print('Classification Accuracy %.2f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Para verificar la calidad de la representación obtenida, implemente *k-means* (un método básico de\n",
    "agrupamiento) sobre la representación obtenida por el autoencoder. Mida la calidad del agrupamiento\n",
    "obtenido sobre los datos reducidos utilizando la métrica denominada ARI (*Adjusted Rand Index*) y\n",
    "la función de desempeño (que llamaremos *clustering accuracy*) definida en el código de ejemplo que se\n",
    "proporciona más abajo. Compare el resultado con el agrupamiento obtenido sobre los datos originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clustering_accuracy(pred_labels,y,nclusters=10):\n",
    "    true_pred = 0.0\n",
    "    for i in range(0,nclusters):\n",
    "        mvlabel = np.argmax(np.bincount(y[pred_labels==i]))\n",
    "        true_pred += sum(y[pred_labels==i] == mvlabel)\n",
    "    return true_pred/len(y)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "model = KMeans(n_clusters=10)\n",
    "labels_pred = model.fit_predict(encoded_train)\n",
    "score = metrics.adjusted_rand_score(y_train, labels_pred)\n",
    "print ('Clustering ARI %.2f' % score)\n",
    "print ('Clustering ACC %.2f' % clustering_accuracy(labels_pred,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) $\\star$ Compare la calidad de la representación reducida obtenida por el autoencoder básico con aquella\n",
    "obtenida vía PCA utilizando el mismo número de dimensiones $d'$. Considere los $4$ criterios que hemos\n",
    "utilizado hasta el momento, i.e., error de reconstrucción, visualización de la reconstrucción, desempeño\n",
    "en clasificación (vía kNN) y desempeño en agrupamiento (vía kMeans). Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pca = PCA(n_components=32)\n",
    "pca.fit(x_train)\n",
    "pca_train = pca.transform(x_train)\n",
    "pca_test = pca.transform(x_test)\n",
    "clf = KNeighborsClassifier(10)\n",
    "clf.fit(pca_train, y_train)\n",
    "score = clf.score(pca_test,y_test)\n",
    "print('PCA SCORE %.2f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Entrene una RBM binaria básica para generar una representación de MNIST en $d' = 2, 8, 32, 64$ dimensiones.\n",
    "Determine el porcentaje de compresión obtenido y el error de reconstrucción en cada caso.\n",
    "Compare con los resultados obtenidos por el autoencoder utilizando los $3$ criterios que hemos utilizado\n",
    "hasta el momento, i.e., error de reconstrucción, desempeño en clasificación (vía kNN) y desempeño en\n",
    "agrupamiento (vía kMeans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "import numpy as np\n",
    "import pickle ##to save trained models\n",
    "model = BernoulliRBM(n_components=32, batch_size=25,learning_rate=0.05,verbose=1, n_iter=50) ##n_components is d\n",
    "model.fit(x_train)##Train using persistent Gibbs chains\n",
    "fileo = open('basicRBM.pickle','wb')\n",
    "pickle.dump(model,fileo)\n",
    "fileo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) $\\star \\star$ Modifique el autoencoder básico construido en (a) para implementar un *deep autoencoder* (deep\n",
    "AE), es decir, un autoencoder con al menos dos capas ocultas. Demuestre experimentalmente que este\n",
    "autoencoder puede mejorar significativamente la compresión obtenida por PCA utilizando el mismo\n",
    "número de dimensiones $d'$. Experimente con $d' = 2, 4, 8, 16, 32$ y distintas profundidades ($L = 2, 3, 4$).\n",
    "Considere en esta comparación los 3 criterios que hemos utilizado hasta el momento, i.e., error de\n",
    "reconstrucción, desempeño en clasificación (vía kNN) y desempeño en agrupamiento (vía kMeans).\n",
    "Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_dim = 2 #try other and do a nice plot\n",
    "input_img = Input(shape=(784,))\n",
    "encoded1 = Dense(1000, activation='relu')(input_img)\n",
    "encoded2 = Dense(500, activation='relu')(encoded1)\n",
    "encoded3 = Dense(250, activation='relu')(encoded2)\n",
    "encoded4 = Dense(target_dim, activation='relu')(encoded3)\n",
    "decoded4 = Dense(250, activation='relu')(encoded4)\n",
    "decoded3 = Dense(500, activation='relu')(encoded3)\n",
    "decoded2 = Dense(1000, activation='relu')(decoded3)\n",
    "decoded1 = Dense(784, activation='sigmoid')(decoded2)\n",
    "autoencoder = Model(input=input_img, output=decoded1)\n",
    "encoder = Model(input=input_img, output=encoded3)\n",
    "autoencoder.compile(optimizer=SGD(lr=1.0), loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train,x_train,nb_epoch=50,batch_size=25,shuffle=True,validation_data=(x_val, x_val))\n",
    "autoencoder.save('my_autoencoder_768x1000x500x250x2.h5')\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "pca = PCA(n_components=target_dim)\n",
    "pca.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) Para el caso $d'=2$ de los experimentos anteriores, genere un gráfico que muestre la representación aprendida. Con este fin, utilice por ejemplo la herramienta de visualización TSNE disponible en *sklearn*. Compare cualitativamente el resultado con aquel obtenido usando PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nplot=5000 #warning: mind your memory!\n",
    "encoded_train = encoder.predict(x_train[:nplot])\n",
    "from sklearn.manifold import TSNE\n",
    "model = TSNE(n_components=2, random_state=0)\n",
    "encoded_train = model.fit_transform(encoded_train)\n",
    "\n",
    "colors={0:'b',1:'g',2:'r',3:'c',4:'m',5:'y',6:'k',7:'orange',8:'darkgreen',9:'maroon'}\n",
    "markers={0:'o',1:'+',2:'v',3:'<',4:'>',5:'^',6:'s',7:'p',8:'*',9:'x'}\n",
    "plt.figure(figsize=(10, 10))\n",
    "for idx in xrange(0,nplot):\n",
    "    label = y_train[idx]\n",
    "    line = plt.plot(encoded_train[idx][0], encoded_train[idx][1],color=colors[label], marker=markers[label], markersize=6)\n",
    "pca_train = pca.transform(x_train)\n",
    "encoded_train = pca_train[:nplot]\n",
    "#... plot PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Contruya una función que permita visualizar algunos de los pesos aprendidos por las neuronas de la primera capa del autoencoder. Muestre el resultado para las mejores redes conseguidas en los ítem anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "j) Estudie como cambian los resultados del modelo construido en (a) si se impone simetría, es decir, si se trabaja con *tied weights*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.2 Denoising </h2>\n",
    "Como hemos discutido en clases, un *denoising autoencoder* (dAE) es esencialmente un *autoencoder* entrenado\n",
    "para reconstruir ejemplos parcialmente corruptos. Varios autores han demostrado que mediante esta modificación simple es posible obtener representaciones latentes más robustas y significativas que aquellas obtenidas por un AE básico. En esta sección exploraremos la aplicación más \"natural\" o \"directa\" del método."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Genere artificialmente una versión corrupta de las imágenes en MNIST utilizando el siguiente modelo de ruido (masking noise): si $x \\in \\mathbb{R}^d$ es una de las imágenes originales, la versión ruidosa $\\tilde x$ se obtiene como $\\tilde x = x \\odot \\xi$ donde $\\odot $ denota el producto de Hadamard ( componente a componente) y $\\xi \\in \\mathbb{R}^d$ es un vector aleatorio binario con componentes Ber($p$) independientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy.random import binomial\n",
    "noise_level = 0.1\n",
    "noise_mask = binomial(n=1,p=noise_level,size=x_train.shape)\n",
    "noisy_x_train = x_train*noise_mask\n",
    "noise_mask = binomial(n=1,p=noise_level,size=x_val.shape)\n",
    "noisy_x_val = x_val*noise_mask\n",
    "noise_mask = binomial(n=1,p=noise_level,size=x_test.shape)\n",
    "noisy_x_test = x_test*noise_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "b) Entrene un autoencoder para recontruir las imágenes corruptas generadas en el ítem anterior. Mida el error de reconstrucción y evalúe cualitativamente (visualización de la imagen corrupta y reconstruida) el resultado para un subconjunto representativo de imágenes. Experimente diferentes valores de $p$ en el rango $(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DEFINE YOUR AUTOENCODER AS BEFORE\n",
    "autoencoder.fit(noisy_x_train, x_train, nb_epoch=50, batch_size=25, shuffle=True, validation_data=(noisy_x_val, x_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Genere artificialmente una versión corrupta de las imágenes en MNIST utilizando el siguiente modelo de ruido ( Gaussian noise): si $x \\in \\mathbb{R}^d$ es una de las imágenes originales, la versión ruidosa $\\tilde x$ se obtiene como $\\tilde = x + \\xi$ donde $\\xi \\in \\mathbb{R}^d$ esun vector aleatorio binario con componentes $N(0,\\sigma ^{2})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy.random import standard_normal\n",
    "devst = 0.5\n",
    "noise_mask = devst*standard_normal(size=x_train.shape)\n",
    "noisy_x_train = x_train*noise_mask\n",
    "noise_mask = devst*standard_normal(size=x_val.shape)\n",
    "\n",
    "noisy_x_val = x_val*noise_mask\n",
    "noise_mask = devst*standard_normal(size=x_test.shape)\n",
    "noisy_x_test = x_test*noise_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Entrene un autoencoder para reconstruir las imágenes corruptas generadas en el ítem anterior. Mida el\n",
    "error de reconstrucción y evalúe cualitativamente (visualización de la imagen corrupta y reconstruida)\n",
    "el resultado para un subconjunto representativo de imágenes. Experimente diferentes valores de \u001b$\\sigma$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Escriba una función que permita visualizar los pesos aprendidos por el $dAE$ y compárelos con aquellos\n",
    "aprendidos por un $AE$ ordinario. ¿Observa diferencias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Suponga que su objetivo es aprender una representación de menor dimensionalidad del conjunto de ejemplos.\n",
    "¿Es posible mejorar los resultados de reconstrucción obtenidos con un AE ordinario entrenándolo\n",
    "con datos artificialmente corruptos? Proyecte un conjunto de experimentos que permita evaluar esta\n",
    "hipótesis. Note que en este caso debe evaluar el AE sobre los datos de prueba no corruptos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>1.3 Pre.entrenamiento</h2>\n",
    "En esta sección utilizaremos los modelos de las secciones anteriores (autoencoders y RBMs) para pre-entrenar\n",
    "redes profundas. Como hemos discutido en clases, el efecto esperado es regularizar el modelo, posicionando\n",
    "el modelo de partida en una buena zona del espacio de parámetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Construya y entrene una red FF para clasificar las imágenes de MNIST. Utilice BP sin ningún tipo\n",
    "de pre-entrenamiento. Para empezar, utilice una arquitectura $768 \\times 1000 \\times 1000 \\times 10$ y funciones de\n",
    "activación sigmoidales. Determine la *accuracy* (fracción de clasificaciones correctas) alcanzada por el\n",
    "modelo en el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## PARAMETERS ...\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'sigmoid'\n",
    "decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer1 = 'sigmoid'\n",
    "decoder_activation_2 = 'sigmoid'\n",
    "loss_ = 'binary_crossentropy'\n",
    "optimizer_ = SGD(lr=1.0)\n",
    "epochs_ = 50 \n",
    "batch_size_ = 25\n",
    "\n",
    "## Load and preprocess MNIST as usual\n",
    "from keras.datasets import mnist\n",
    "## HERE YOU NEED: Y_train,Y_val,Y_test produced in 1(a)\n",
    "\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)))\n",
    "model.add(Dense(n_hidden_layer2, activation=activation_layer2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=optimizer_,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, Y_train,nb_epoch=50, batch_size=25,shuffle=True, validation_data=(x_val, Y_val))\n",
    "model.save('ReluNet-768x1000x1000x10-NFT-50epochs.h5') #USEFUL WHEN TRAINING IS SLOW\n",
    "#TRAINING CAN THEN BE RESUMED FROM THIS POINT :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Construya y entrene una red neuronal profunda para clasificar las imágenes de MNIST utilizando la\n",
    "arquitectura propuesta en (a) y pre-entrenando los pesos de cada capa mediante un autoencoder básico.\n",
    "Proceda en modo clásico, es decir, entrenando en modo no-supervisado una capa a la vez y tomando\n",
    "como input de cada nivel la representación (entrenada) obtenida en el nivel anterior. Después del entrenamiento\n",
    "efectúe un entrenamiento supervisado convencional (*finetunning*). Compare los resultados de\n",
    "clasificación sobre el conjunto de pruebas con aquellos obtenidos en (a), sin pre-entrenamiento. Evalúe\n",
    "también los resultados antes del *finetunning*. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Load and preprocess MNIST as usual\n",
    "from keras.datasets import mnist\n",
    "\n",
    "###AUTOENCODER 1\n",
    "input_img1 = Input(shape=(784,))\n",
    "encoded1 = Dense(n_hidden_layer1,activation=activation_layer1)(input_img1)\n",
    "decoded1 = Dense(784, activation=decoder_activation_1)(encoded1)\n",
    "autoencoder1 = Model(input=input_img1, output=decoded1)\n",
    "encoder1 = Model(input=input_img1, output=encoded1)\n",
    "autoencoder1.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder1.fit(x_train, x_train, nb_epoch=epochs_, batch_size=batch_size_,shuffle=True, validation_data=(x_val, x_val))\n",
    "encoded_input1 = Input(shape=(n_hidden_layer1,))\n",
    "autoencoder1.save('autoencoder_layer1.h5')\n",
    "encoder1.save('encoder_layer1.h5')\n",
    "\n",
    "###AUTOENCODER 2\n",
    "x_train_encoded1 = encoder1.predict(x_train) #FORWARD PASS DATA THROUGH FIRST ENCODER\n",
    "x_val_encoded1 = encoder1.predict(x_val)\n",
    "x_test_encoded1 = encoder1.predict(x_test)\n",
    "\n",
    "input_img2 = Input(shape=(n_hidden_layer1,))\n",
    "encoded2 = Dense(n_hidden_layer2, activation=activation_layer2)(input_img2)\n",
    "decoded2 = Dense(n_hidden_layer2, activation=decoder_activation_2)(encoded2)\n",
    "autoencoder2 = Model(input=input_img2, output=decoded2)\n",
    "encoder2 = Model(input=input_img2, output=encoded2)\n",
    "autoencoder2.compile(optimizer=optimizer_, loss=loss_)\n",
    "autoencoder2.fit(x_train_encoded1,x_train_encoded1,nb_epoch=epochs_,batch_size=batch_size_,shuffle=True, validation_data=(x_val_encoded1, x_val_encoded1))\n",
    "encoded_input2 = Input(shape=(n_hidden_layer2,))\n",
    "autoencoder2.save('autoencoder_layer2.h5')\n",
    "encoder2.save('encoder_layer2.h5')\n",
    "\n",
    "#FINE TUNNING\n",
    "from keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Dense(n_hidden_layer1, activation=activation_layer1, input_shape=(784,)))\n",
    "model.layers[-1].set_weights(autoencoder1.layers[1].get_weights())\n",
    "model.add(Dense(n_hidden_layer2, activation=activation_layer2))\n",
    "model.layers[-1].set_weights(autoencoder2.layers[1].get_weights())\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.summary()\n",
    "model.compile(optimizer=optimizer_,loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, Y_train,nb_epoch=20, batch_size=25,shuffle=True, validation_data=(x_val, Y_val))\n",
    "model.save('Net-768x1000x1000x10-finetunned.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Construya y entrene una red neuronal profunda para clasificar las imágenes de MNIST utilizando la\n",
    "arquitectura propuesta en (a) y pre-entrenando los pesos de cada capa mediante una RBM binaria\n",
    "básica. Compare los resultados con aquellos obtenidos en (a) y (b). Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import BernoulliRBM\n",
    "rbm1 = BernoulliRBM(n_components=n_hidden_layer1, batch_size=25,learning_rate=0.05,verbose=1, n_iter=50)\n",
    "rbm1.fit(x_train)##Train using persistent Gibbs chains\n",
    "encoded_train1 = rbm1.transform(x_train)\n",
    "encoded_val1 = rbm1.transform(x_val)\n",
    "encoded_test1 = rbm1.transform(x_test)\n",
    "\n",
    "rbm2 = BernoulliRBM(n_components=n_hidden_layer2, batch_size=25,learning_rate=0.05,verbose=1, n_iter=50)\n",
    "rbm2.fit(encoded_train1)\n",
    "encoded_train2 = rbm2.transform(encoded_train1)\n",
    "encoded_val2 = rbm2.transform(encoded_val1)\n",
    "encoded_test2 = rbm2.transform(encoded_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Construya y entrene una red neuronal profunda para clasificar las imágenes de MNIST utilizando la arquitectura\n",
    "propuesta en (a) y pre-entrenando los pesos de cada capa mediante un *denoising autoencoder.*\n",
    "Compare los resultados con aquellos obtenidos en (a), (b) y (c). Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e)Evalúe el efecto de incorporar un regulador $\\ell_{2}$ y/o $\\ell_{1}$ (elija usted) al enternamiento de la red neuronal final. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) $\\star$ Repita los experimentos (a)-(d) utilizando funciones de activación $Tanh$ y $ReLU$. comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## PARAMETERS ...\n",
    "n_hidden_layer1 = 1000\n",
    "activation_layer1 = 'relu'\n",
    "decoder_activation_1 = 'sigmoid'\n",
    "n_hidden_layer2 = 1000\n",
    "activation_layer1 = 'relu'\n",
    "decoder_activation_2 = 'sigmoid'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g) $\\star \\star$ Evalúe el efecto de cambiar el número de neuronas ocultas en cada capa del modelo. Por simplicidad y aún si no es la arquitectura óptima para este problema puede fijar el número de capas ocultas a $L=3$ y experimente, al menos, con un número de neuronas igual a $500, 1000, 2000, 4000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h) $\\star \\star$ Evalúe el efecto de aumentar la profundidad de $1, 2, 3$ niveles en el modelo. Por simplicidad y aún si no es la arquitectura óptima para este problema mantena fijo el número de neuronas ocultas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>2 Aprendizaje Semi.Supervisado en NORB</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como hemos discutido en clases, uno de los problemas más relevantes a la hora de aplicar técnicas de aprendizaje\n",
    "automático a problemas es reales es el requisito de disponer de un gran número de datos etiquetados,\n",
    "es decir, ejemplos para los que se conoce la respuesta deseada del sistema. Un problema de aprendizaje para\n",
    "el que existen pocos datos etiquetados y muchos datos no etiquetados se denomina *semi-supervisado*. En\n",
    "esta sección, utilizaremos la idea de pre-entrenar una red en modo no supervisado para atacar problemas\n",
    "de aprendizaje semi-supervisado. Con este objetivo en mente, trabajaremos con un dataset denominado\n",
    "$NORB$, introducido en [9] y utilizado en [10], que corresponde a imágenes estéreo de juguetes clasificados en\n",
    "$6$ categorías. Se tienen $291.600$ ejemplos de entrenamiento y $58.320$ ejemplos de pruebas.\n",
    "<img src=\"img/Fig2.png\">\n",
    "<center>Fig. 2: Dataset NORB.</center>\n",
    "Los datos asociados a esta actividad podrán ser obtenidos utilizando los siguientes comandos en la línea\n",
    "de comandos (sistemas UNIX)\n",
    "```\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_1\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_2\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_3\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_4\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_5\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_6\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_7\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_8\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_9\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_10\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_11\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_12\n",
    "wget http://octopus.inf.utfsm.cl/~ricky/data_batch_13\n",
    "```\n",
    "Los primeros $10$ batches corresponden a los datos de entrenamiento y los últimos $2$ a los datos de pruebas. Los\n",
    "archivos corresponden a diccionarios serializados de python y pueden ser \"extraídos\" utilizando la siguiente\n",
    "función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import cPickle\n",
    "    fo = open(file, rb)\n",
    "    dict = cPickle.load(fo)\n",
    "    fo.close()\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez extraído, cada diccionario contendrá $2$ elementos importantes: *data* y *labels*. El primer elemento\n",
    "(*data*) es un matriz de $2048 \\times n$ (numpy array). Cada columna de esa matriz corresponde a una imagen\n",
    "estéreo de un juguete: los primeros $1024$ valores vienen de una de las cámaras/vistas y los siguientes $1024$ de\n",
    "la otra. Por otro lado, el elemento (*labels*) del diccionario contiene una lista de $n$ valores enteros entre $0$ y $5$ que identifican las clases antes a las que pertenecen los juguetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Construya una función que cargue todos los bloques de entrenamiento y pruebas del problema NORB\n",
    "generando como salida: <br>\n",
    "(i) dos matrices $X_{tr}, Y_{tr}$, correspondientes a las imágenes y etiquetas de entrenamiento, <br>\n",
    "(ii) dos matrices $X_t, Y_t$, correspondientes a las imágenes y etiquetas de pruebas, y finalmente<br>\n",
    "(iii) dos matrices $X_v, Y_v$, correspondientes a imágenes y etiquetas que se usarán como conjunto de\n",
    "validación, es decir para tomar decisiones de diseño acerca del modelo. Este último conjunto debe ser\n",
    "extraído desde el conjunto de entrenamiento seleccionando $5.832$ casos de cada batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_NORB_train_val(PATH):\n",
    "    xtr = []\n",
    "    ytr = []\n",
    "    xval = []\n",
    "    yval = []\n",
    "\n",
    "    for b in range(1,11):\n",
    "        f = os.path.join(PATH,'data_batch_%d' % (b, ))\n",
    "        datadict = unpickle(f)\n",
    "        X = datadict['data'].T\n",
    "        Y = np.array(datadict['labels'])\n",
    "        Z = np.concatenate((X,Y),axis=1)\n",
    "        Z = np.random.shuffle(Z)\n",
    "        xtr.append(Z[5832:,0:-1])\n",
    "        ytr.append(Z[5832:,-1])\n",
    "        xval.append(Z[:5832,0:-1])\n",
    "        yval.append(Z[:5832,-1])\n",
    "\n",
    "    Xtr = np.concatenate(xtr)\n",
    "    Ytr = np.concatenate(ytr)\n",
    "    Xval = np.concatenate(xval)\n",
    "    Yval = np.concatenate(yval)\n",
    "    del xtr,ytr,xval,yval\n",
    "    return Xtr, Ytr, Xval, Yval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Construya una función que escale apropiadamente las imágenes antes de trabajar. Experimente escalando\n",
    "linealmente los datos de tal forma que cada pixel quede en el intervalo $[-1,1]$ con el máximo y\n",
    "mínimo valor observado en los extremos del intervalo. Evalúe más tarde la ventaja de centrar y escalar\n",
    "los datos para que cada atributo (pixel) tenga desviación estándar $1$ y media nula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Su objetivo será ahora evaluar el desempeño de una red FF en un escenario semi-supervisado. Para\n",
    "ello simulará un situación en la que se tienen $n_s$ ejemplos de entrenamiento para los cuales se conoce\n",
    "la etiqueta correcta y $n_{ns} = n_{tr} - n_s$ ejemplos para los cuales no se tiene esta información ($n_{tr}$ es el\n",
    "número total de ejemplos de entrenamiento). Para empezar, deberá entrenar una red FF con salida\n",
    "softmax para el problema NORB. Considere la inclusión de dos capas escondidas (de $4000$ y $2000$\n",
    "unidades) y funciones de activación $relu$. Como parámetros de referencia considere: BP con tasa de\n",
    "aprendizaje constante, función de pérdida *cross-entropy binaria*, y mini batches de tamaño $10$. Puede\n",
    "utilizar el conjunto de validación para mejorar el entrenamiento. Construya un gráfico que muestre\n",
    "cómo evoluciona el error de pruebas como función de $\\theta_s = n_s/n_{tr}$. Experimente con $\\theta_s = 0.1, 0.2,...1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Su objetivo será ahora construir un gráfico similar al anterior que muestre cómo evoluciona el error de pruebas como función de $\\theta_s = n_s / n_{tr}$ cuando la red se pre-entrena utilizando los datos no supervisados. ¿Mejora el resultado con respecto a la red entrenada utilizando sólo los casos para los que se conoce la etiqueta? Experimente pre-entrenando con distintas estrategias ( por ejemplo AE's versus dAe's ó AE's versus RBM's)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Repita el experimento anterior cambiando las funciones de activación a $sigmoidales$ y $tanh$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python35]",
   "language": "python",
   "name": "Python [python35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
